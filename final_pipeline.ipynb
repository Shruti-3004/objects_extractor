{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping already processed video: /Users/shrutiagarwal/Desktop/objects_extractor/videos/maybelline.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------- Configuration ---------------------------- #\n",
    "\n",
    "# Load environment variables (here, OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client (uses key from env variable)\n",
    "client = OpenAI()\n",
    "\n",
    "# Output CSV file\n",
    "OUTPUT_CSV = \"detected_objects.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Setup: Load Previous Results ---------------------------- #\n",
    "\n",
    "# Load existing results if any\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "    processed_videos = set(existing_df['video'].tolist())\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"video\", \"products\"])\n",
    "    processed_videos = set()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Scene Change Detection ---------------------------- #\n",
    "\n",
    "def is_scene_change(prev_frame, curr_frame, threshold):\n",
    "    \"\"\"\n",
    "    Compares two grayscale frames using histogram correlation.\n",
    "    Returns True if the similarity is below the threshold, indicating a scene change.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute grayscale histograms for both frames\n",
    "    prev_hist = cv2.calcHist([prev_frame], [0], None, [256], [0, 256])\n",
    "    curr_hist = cv2.calcHist([curr_frame], [0], None, [256], [0, 256])\n",
    "    \n",
    "    # Normalize the histograms so they are comparable\n",
    "    cv2.normalize(prev_hist, prev_hist)\n",
    "    cv2.normalize(curr_hist, curr_hist)\n",
    "\n",
    "    # Compare histograms using correlation; closer to 1 means similar\n",
    "    similarity = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n",
    "\n",
    "    # If similarity is below threshold, it indicates a scene change\n",
    "    return similarity < threshold\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Function to convert image to base64 --------------------- #\n",
    "\n",
    "def image_to_base64(image):\n",
    "    \"\"\"\n",
    "    Converts a CV2 image to a base64-encoded JPEG string.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): OpenCV image (BGR format).\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64-encoded string of the JPEG image, or None if conversion fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Encode the image to JPEG format\n",
    "        # The first element of the tuple is a success flag, the second is the encoded image buffer\n",
    "        retval, buffer = cv2.imencode('.jpg', image)\n",
    "        if not retval:\n",
    "            raise ValueError(\"Image encoding to JPEG failed.\")\n",
    "\n",
    "        # Convert the buffer (NumPy array) to bytes\n",
    "        jpg_as_bytes = buffer.tobytes()\n",
    "\n",
    "        # Encode the bytes to Base64\n",
    "        jpg_as_base64 = base64.b64encode(jpg_as_bytes)\n",
    "\n",
    "        # Convert to base64 string (not bytes)\n",
    "        jpg_as_string = jpg_as_base64.decode('utf-8')\n",
    "\n",
    "        return jpg_as_string\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"⚠️ Error in image_to_base64: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Vision Model (OpenAI) ---------------------------- #\n",
    "\n",
    "def identify_objects(frame):\n",
    "    \"\"\"\n",
    "    Sends an image frame to OpenAI Vision model to detect branded products.\n",
    "\n",
    "    Args:\n",
    "        frame (np.ndarray): BGR image frame from OpenCV.\n",
    "\n",
    "    Returns:\n",
    "        str: a string of identified products, or 'ERROR' if failed.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Prompt to use ===\n",
    "    prompt = (\n",
    "        \"Identify and list all branded or commercial products visible in this image.\"\n",
    "        \"Don't give a complete description of what you have identified. Just return the product with brand name.\"\n",
    "        \"for example - 'Pepsi can', 'Doritos chips', 'Reebok tracksuit', 'Fruity loops cereal', etc\"\n",
    "        # \"include the full product name like 'Maybelline Fit Me Blush' or 'Maybelline Super Stay Foundation'. Avoid generic brand names like 'Maybelline Fit Me' without stating what the product is. Do not guess, but infer based on visible packaging and design.\"\n",
    "        # \"Pay attention to packaging — tube, bottle, compact, etc. Avoid assuming based on product line names like 'Fit Me' or 'Dewy + Smooth'.\"\n",
    "        \"Use your best judgment, but avoid listing products that are not visibly present. \"\n",
    "        \"If a product is partially obscured but clearly identifiable by packaging, include it. \"\n",
    "        \"Do not guess products that are not visually indicated.\"\n",
    "        \"If there is no product visible in the image, simply return None\"\n",
    "    )\n",
    "\n",
    "    image_b64 = image_to_base64(frame) #  Convert to base64 for API\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", # Vision-capable model\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
    "                    }}\n",
    "                ]}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = None\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Output Cleaning ---------------------------- #\n",
    "\n",
    "def get_final_products(all_products):\n",
    "    \"\"\"\n",
    "    Deduplicates and returns a clean string of unique products.\n",
    "\n",
    "    Args:\n",
    "        product_list (list of str): Raw model responses.\n",
    "\n",
    "    Returns:\n",
    "        str: Sorted, comma-separated list of unique products.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt2 = f'''\n",
    "\n",
    "    You are given a list or block of text that may contain repeated, messy, or slightly varied product names.\n",
    "\n",
    "    Your task is to extract a clean, alphabetically sorted list of unique product names from it.\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    Ignore values like \"None\", \"N/A\", or empty entries.\n",
    "\n",
    "    Split multiple products if they appear in one string separated by commas or dashes.\n",
    "\n",
    "    Normalize duplicates with slight variations. For example, \"Maybelline Master Chrome\" and \"Maybelline Master Chrome Highlighter\" should be merged as \"Maybelline Master Chrome Highlighter\".\n",
    "\n",
    "    Do not include vague labels like just \"Maybelline Fit Me\" — prefer specific ones like \"Maybelline Fit Me Blush\" or \"Maybelline Fit Me Dewy + Smooth Primer\".\n",
    "\n",
    "    Do not make up products. Only include what's visibly or clearly listed.\n",
    "\n",
    "    Return the final result as a plain list of comma separated strings (not as a JSON object or explanation).\n",
    "\n",
    "    List: {all_products}\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt2    \n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2\n",
    "                )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Main Pipeline ---------------------------- #\n",
    "\n",
    "vid_dir = os.path.join(os.getcwd(), 'videos')\n",
    "\n",
    "for vid in glob.glob(vid_dir+'/*.mp4'):\n",
    "\n",
    "    if vid in processed_videos:\n",
    "        print(f\"✅ Skipping already processed video: {vid}\")\n",
    "        continue\n",
    "\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(vid)\n",
    "        prev_frame = None # Store previous frame for comparison\n",
    "        frame_id = 0 # Track the current frame number\n",
    "        scene_id = 0 # Track the number of detected scenes\n",
    "\n",
    "        # Create a directory to save scene images\n",
    "        # os.makedirs('scenes')\n",
    "\n",
    "        # Loop through video frames\n",
    "        while True:\n",
    "            ret, frame = cap.read() # Read next frame\n",
    "            if not ret:\n",
    "                break # Exit loop if video ends or fails to read\n",
    "\n",
    "            # Convert current frame to grayscale for histogram analysis\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Skip comparison for the first frame\n",
    "            if prev_frame is None:\n",
    "                prev_frame = gray\n",
    "                continue\n",
    "\n",
    "            # Detect scene change by comparing current and previous frame\n",
    "            if is_scene_change(prev_frame, gray, threshold=0.8):\n",
    "                print(f\"Scene changed at frame {frame_id}\")\n",
    "\n",
    "                objs = identify_objects(frame)\n",
    "                all_products.append(objs)\n",
    "\n",
    "                # # Save frame as an image in the output directory, uncomment to refer to images of scenes saved\n",
    "                # cv2.imwrite(f\"scenes/scene_{scene_id:03d}.jpg\", frame)\n",
    "\n",
    "                scene_id += 1\n",
    "            \n",
    "            # Update previous frame and frame counter\n",
    "            prev_frame = gray\n",
    "            frame_id += 1\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "        # Get final list of products and update CSV\n",
    "        final_output = get_final_products(all_products)\n",
    "        new_entry = pd.DataFrame([[vid, final_output]], columns=[\"video\", \"products\"])\n",
    "        existing_df = pd.concat([existing_df, new_entry], ignore_index=True)\n",
    "        existing_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {vid_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delphi-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
